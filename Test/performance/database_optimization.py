#!/usr/bin/env python3
"""
Database Optimization Script for Sentiment Analysis & Decision Support System
Optimizes database performance through connection pooling, indexing, and query optimization.
"""

import time
import json
import logging
from typing import Dict, Any, List
import os
import sys
from datetime import datetime
import sqlite3
from contextlib import contextmanager

# Add project root to path
project_root = os.path.dirname(
    os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
)
sys.path.append(project_root)

from src.core.storage.database_manager import DatabaseManager

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DatabaseOptimizer:
    """Database optimization utilities for the decision support system."""
    
    def __init__(self, db_path: str = "data/decision_support.db"):
        self.db_path = db_path
        self.optimization_results = {
            "test_start": datetime.now().isoformat(),
            "optimizations": {},
            "summary": {}
        }
        self.db_manager = DatabaseManager()
    
    def setup_connection_pooling(self) -> Dict[str, Any]:
        """Set up and test connection pooling."""
        logger.info("Setting up connection pooling")
        
        import queue
        import threading
        
        class ConnectionPool:
            def __init__(self, db_path: str, max_connections: int = 10):
                self.db_path = db_path
                self.max_connections = max_connections
                self.pool = queue.Queue(maxsize=max_connections)
                self.lock = threading.Lock()
                self.active_connections = 0
                
                # Initialize pool with connections
                for _ in range(max_connections):
                    conn = sqlite3.connect(db_path)
                    conn.row_factory = sqlite3.Row
                    self.pool.put(conn)
            
            @contextmanager
            def get_connection(self):
                """Get a connection from the pool."""
                conn = self.pool.get()
                try:
                    yield conn
                finally:
                    self.pool.put(conn)
            
            def close_all(self):
                """Close all connections in the pool."""
                while not self.pool.empty():
                    conn = self.pool.get()
                    conn.close()
        
        # Test different pool sizes
        pool_sizes = [5, 10, 20, 50]
        results = {}
        
        for pool_size in pool_sizes:
            logger.info(f"Testing connection pool with {pool_size} connections")
            
            # Create pool
            pool = ConnectionPool(self.db_path, pool_size)
            
            # Test concurrent access
            start_time = time.time()
            threads = []
            
            def worker():
                with pool.get_connection() as conn:
                    cursor = conn.cursor()
                    cursor.execute("SELECT COUNT(*) FROM knowledge_graph_entities")
                    result = cursor.fetchone()
                    time.sleep(0.01)  # Simulate work
            
            # Create threads
            for _ in range(pool_size * 2):  # More threads than connections
                thread = threading.Thread(target=worker)
                threads.append(thread)
                thread.start()
            
            # Wait for all threads to complete
            for thread in threads:
                thread.join()
            
            total_time = time.time() - start_time
            
            # Clean up
            pool.close_all()
            
            results[f"pool_size_{pool_size}"] = {
                "pool_size": pool_size,
                "total_time": total_time,
                "avg_time_per_request": total_time / (pool_size * 2),
                "requests_per_second": (pool_size * 2) / total_time
            }
        
        return results
    
    def optimize_database_indexes(self) -> Dict[str, Any]:
        """Optimize database indexes for better query performance."""
        logger.info("Optimizing database indexes")
        
        # Define index optimization strategies
        index_strategies = {
            "knowledge_graph_entities": [
                "CREATE INDEX IF NOT EXISTS idx_entities_type ON knowledge_graph_entities(entity_type)",
                "CREATE INDEX IF NOT EXISTS idx_entities_name ON knowledge_graph_entities(entity_name)",
                "CREATE INDEX IF NOT EXISTS idx_entities_confidence ON knowledge_graph_entities(confidence_score)",
                "CREATE INDEX IF NOT EXISTS idx_entities_timestamp ON knowledge_graph_entities(created_at)"
            ],
            "decision_patterns": [
                "CREATE INDEX IF NOT EXISTS idx_patterns_category ON decision_patterns(pattern_category)",
                "CREATE INDEX IF NOT EXISTS idx_patterns_confidence ON decision_patterns(confidence_score)",
                "CREATE INDEX IF NOT EXISTS idx_patterns_timestamp ON decision_patterns(created_at)"
            ],
            "scenario_analysis": [
                "CREATE INDEX IF NOT EXISTS idx_scenarios_type ON scenario_analysis(scenario_type)",
                "CREATE INDEX IF NOT EXISTS idx_scenarios_priority ON scenario_analysis(priority_score)",
                "CREATE INDEX IF NOT EXISTS idx_scenarios_timestamp ON scenario_analysis(created_at)"
            ]
        }
        
        results = {}
        
        for table_name, indexes in index_strategies.items():
            logger.info(f"Optimizing indexes for table: {table_name}")
            
            table_results = {
                "indexes_created": 0,
                "creation_times": [],
                "query_improvements": {}
            }
            
            # Create indexes and measure time
            for index_sql in indexes:
                start_time = time.time()
                try:
                    self.db_manager.execute_query(index_sql)
                    creation_time = time.time() - start_time
                    table_results["creation_times"].append(creation_time)
                    table_results["indexes_created"] += 1
                except Exception as e:
                    logger.error(f"Failed to create index: {e}")
            
            # Test query performance improvements
            query_tests = self._get_query_tests(table_name)
            
            for test_name, query in query_tests.items():
                # Test without index
                start_time = time.time()
                self.db_manager.execute_query(query)
                time_without_index = time.time() - start_time
                
                # Test with index (after creating indexes)
                start_time = time.time()
                self.db_manager.execute_query(query)
                time_with_index = time.time() - start_time
                
                improvement = ((time_without_index - time_with_index) / time_without_index) * 100
                
                table_results["query_improvements"][test_name] = {
                    "time_without_index": time_without_index,
                    "time_with_index": time_with_index,
                    "improvement_percent": improvement
                }
            
            results[table_name] = table_results
        
        return results
    
    def _get_query_tests(self, table_name: str) -> Dict[str, str]:
        """Get test queries for a specific table."""
        query_tests = {
            "knowledge_graph_entities": {
                "select_by_type": "SELECT * FROM knowledge_graph_entities WHERE entity_type = 'PERSON' LIMIT 100",
                "select_by_confidence": "SELECT * FROM knowledge_graph_entities WHERE confidence_score > 0.8 LIMIT 100",
                "select_recent": "SELECT * FROM knowledge_graph_entities WHERE created_at > datetime('now', '-1 day') LIMIT 100",
                "count_by_type": "SELECT entity_type, COUNT(*) FROM knowledge_graph_entities GROUP BY entity_type"
            },
            "decision_patterns": {
                "select_by_category": "SELECT * FROM decision_patterns WHERE pattern_category = 'STRATEGIC' LIMIT 100",
                "select_by_confidence": "SELECT * FROM decision_patterns WHERE confidence_score > 0.7 LIMIT 100",
                "select_recent": "SELECT * FROM decision_patterns WHERE created_at > datetime('now', '-1 day') LIMIT 100",
                "count_by_category": "SELECT pattern_category, COUNT(*) FROM decision_patterns GROUP BY pattern_category"
            },
            "scenario_analysis": {
                "select_by_type": "SELECT * FROM scenario_analysis WHERE scenario_type = 'RISK' LIMIT 100",
                "select_by_priority": "SELECT * FROM scenario_analysis WHERE priority_score > 0.6 LIMIT 100",
                "select_recent": "SELECT * FROM scenario_analysis WHERE created_at > datetime('now', '-1 day') LIMIT 100",
                "count_by_type": "SELECT scenario_type, COUNT(*) FROM scenario_analysis GROUP BY scenario_type"
            }
        }
        
        return query_tests.get(table_name, {})
    
    def implement_query_optimization(self) -> Dict[str, Any]:
        """Implement and test query optimization techniques."""
        logger.info("Implementing query optimization")
        
        # Define optimized queries
        query_optimizations = {
            "avoid_select_star": {
                "original": "SELECT * FROM knowledge_graph_entities WHERE entity_type = 'PERSON'",
                "optimized": "SELECT entity_id, entity_name, entity_type FROM knowledge_graph_entities WHERE entity_type = 'PERSON'"
            },
            "use_limit": {
                "original": "SELECT * FROM decision_patterns WHERE pattern_category = 'STRATEGIC'",
                "optimized": "SELECT * FROM decision_patterns WHERE pattern_category = 'STRATEGIC' LIMIT 100"
            },
            "use_indexed_columns": {
                "original": "SELECT * FROM scenario_analysis WHERE scenario_type = 'RISK' AND priority_score > 0.5",
                "optimized": "SELECT * FROM scenario_analysis WHERE priority_score > 0.5 AND scenario_type = 'RISK'"
            },
            "avoid_subqueries": {
                "original": "SELECT * FROM knowledge_graph_entities WHERE entity_id IN (SELECT entity_id FROM decision_patterns)",
                "optimized": "SELECT DISTINCT e.* FROM knowledge_graph_entities e JOIN decision_patterns d ON e.entity_id = d.entity_id"
            }
        }
        
        results = {}
        
        for optimization_name, queries in query_optimizations.items():
            logger.info(f"Testing optimization: {optimization_name}")
            
            # Test original query
            start_time = time.time()
            try:
                self.db_manager.execute_query(queries["original"])
                original_time = time.time() - start_time
            except Exception as e:
                logger.error(f"Original query failed: {e}")
                original_time = float('inf')
            
            # Test optimized query
            start_time = time.time()
            try:
                self.db_manager.execute_query(queries["optimized"])
                optimized_time = time.time() - start_time
            except Exception as e:
                logger.error(f"Optimized query failed: {e}")
                optimized_time = float('inf')
            
            # Calculate improvement
            if original_time != float('inf') and optimized_time != float('inf'):
                improvement = ((original_time - optimized_time) / original_time) * 100
            else:
                improvement = 0
            
            results[optimization_name] = {
                "original_time": original_time,
                "optimized_time": optimized_time,
                "improvement_percent": improvement,
                "original_query": queries["original"],
                "optimized_query": queries["optimized"]
            }
        
        return results
    
    def setup_database_monitoring(self) -> Dict[str, Any]:
        """Set up database monitoring and performance tracking."""
        logger.info("Setting up database monitoring")
        
        class DatabaseMonitor:
            def __init__(self, db_manager):
                self.db_manager = db_manager
                self.query_stats = {
                    "total_queries": 0,
                    "slow_queries": 0,
                    "failed_queries": 0,
                    "query_times": [],
                    "query_types": {}
                }
                self.performance_metrics = {
                    "avg_query_time": 0,
                    "max_query_time": 0,
                    "min_query_time": float('inf'),
                    "slow_query_threshold": 1.0  # seconds
                }
            
            def execute_monitored_query(self, query: str, query_type: str = "unknown") -> Any:
                """Execute a query with monitoring."""
                start_time = time.time()
                
                try:
                    result = self.db_manager.execute_query(query)
                    query_time = time.time() - start_time
                    
                    # Update statistics
                    self.query_stats["total_queries"] += 1
                    self.query_stats["query_times"].append(query_time)
                    
                    if query_type not in self.query_stats["query_types"]:
                        self.query_stats["query_types"][query_type] = 0
                    self.query_stats["query_types"][query_type] += 1
                    
                    # Check for slow queries
                    if query_time > self.performance_metrics["slow_query_threshold"]:
                        self.query_stats["slow_queries"] += 1
                        logger.warning(f"Slow query detected: {query} ({query_time:.3f}s)")
                    
                    # Update performance metrics
                    self.performance_metrics["avg_query_time"] = sum(self.query_stats["query_times"]) / len(self.query_stats["query_times"])
                    self.performance_metrics["max_query_time"] = max(self.performance_metrics["max_query_time"], query_time)
                    self.performance_metrics["min_query_time"] = min(self.performance_metrics["min_query_time"], query_time)
                    
                    return result
                    
                except Exception as e:
                    self.query_stats["failed_queries"] += 1
                    logger.error(f"Query failed: {query} - {e}")
                    raise
            
            def get_monitoring_stats(self) -> Dict[str, Any]:
                """Get current monitoring statistics."""
                return {
                    "query_stats": self.query_stats,
                    "performance_metrics": self.performance_metrics,
                    "success_rate": ((self.query_stats["total_queries"] - self.query_stats["failed_queries"]) / self.query_stats["total_queries"]) * 100 if self.query_stats["total_queries"] > 0 else 0
                }
        
        # Create monitor
        monitor = DatabaseMonitor(self.db_manager)
        
        # Test queries with monitoring
        test_queries = [
            ("SELECT COUNT(*) FROM knowledge_graph_entities", "count"),
            ("SELECT * FROM knowledge_graph_entities LIMIT 10", "select"),
            ("SELECT entity_type, COUNT(*) FROM knowledge_graph_entities GROUP BY entity_type", "group_by"),
            ("SELECT * FROM decision_patterns WHERE pattern_category = 'STRATEGIC' LIMIT 5", "filtered_select"),
            ("SELECT * FROM scenario_analysis ORDER BY priority_score DESC LIMIT 5", "ordered_select")
        ]
        
        for query, query_type in test_queries:
            try:
                monitor.execute_monitored_query(query, query_type)
            except Exception as e:
                logger.error(f"Test query failed: {e}")
        
        # Get monitoring results
        monitoring_results = monitor.get_monitoring_stats()
        
        return {
            "monitoring_setup": "Database monitoring successfully configured",
            "test_results": monitoring_results,
            "recommendations": self._generate_monitoring_recommendations(monitoring_results)
        }
    
    def _generate_monitoring_recommendations(self, stats: Dict[str, Any]) -> List[str]:
        """Generate recommendations based on monitoring statistics."""
        recommendations = []
        
        # Check success rate
        success_rate = stats.get("success_rate", 0)
        if success_rate < 95:
            recommendations.append("Database success rate below 95% - investigate failed queries")
        
        # Check for slow queries
        slow_queries = stats.get("query_stats", {}).get("slow_queries", 0)
        if slow_queries > 0:
            recommendations.append(f"Found {slow_queries} slow queries - consider query optimization")
        
        # Check average query time
        avg_query_time = stats.get("performance_metrics", {}).get("avg_query_time", 0)
        if avg_query_time > 0.5:
            recommendations.append("Average query time is high - consider indexing or query optimization")
        
        # Check query distribution
        query_types = stats.get("query_stats", {}).get("query_types", {})
        if len(query_types) > 10:
            recommendations.append("High variety of query types - consider query standardization")
        
        return recommendations
    
    def configure_backup_and_recovery(self) -> Dict[str, Any]:
        """Configure backup and recovery procedures."""
        logger.info("Configuring backup and recovery procedures")
        
        import shutil
        from datetime import datetime, timedelta
        
        class DatabaseBackup:
            def __init__(self, db_path: str, backup_dir: str = "backups"):
                self.db_path = db_path
                self.backup_dir = backup_dir
                os.makedirs(backup_dir, exist_ok=True)
            
            def create_backup(self) -> str:
                """Create a backup of the database."""
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                backup_path = os.path.join(self.backup_dir, f"backup_{timestamp}.db")
                
                try:
                    shutil.copy2(self.db_path, backup_path)
                    logger.info(f"Backup created: {backup_path}")
                    return backup_path
                except Exception as e:
                    logger.error(f"Backup failed: {e}")
                    raise
            
            def restore_backup(self, backup_path: str) -> bool:
                """Restore database from backup."""
                try:
                    shutil.copy2(backup_path, self.db_path)
                    logger.info(f"Database restored from: {backup_path}")
                    return True
                except Exception as e:
                    logger.error(f"Restore failed: {e}")
                    return False
            
            def cleanup_old_backups(self, days_to_keep: int = 7) -> int:
                """Clean up old backup files."""
                cutoff_date = datetime.now() - timedelta(days=days_to_keep)
                deleted_count = 0
                
                for filename in os.listdir(self.backup_dir):
                    if filename.startswith("backup_") and filename.endswith(".db"):
                        file_path = os.path.join(self.backup_dir, filename)
                        file_time = datetime.fromtimestamp(os.path.getctime(file_path))
                        
                        if file_time < cutoff_date:
                            try:
                                os.remove(file_path)
                                deleted_count += 1
                                logger.info(f"Deleted old backup: {filename}")
                            except Exception as e:
                                logger.error(f"Failed to delete backup {filename}: {e}")
                
                return deleted_count
        
        # Create backup manager
        backup_manager = DatabaseBackup(self.db_path)
        
        # Test backup and recovery
        backup_results = {
            "backup_created": False,
            "backup_path": None,
            "restore_tested": False,
            "cleanup_performed": False
        }
        
        try:
            # Create backup
            backup_path = backup_manager.create_backup()
            backup_results["backup_created"] = True
            backup_results["backup_path"] = backup_path
            
            # Test restore (create a test copy first)
            test_db_path = self.db_path + ".test"
            shutil.copy2(self.db_path, test_db_path)
            
            # Restore to test location
            test_backup = DatabaseBackup(test_db_path)
            restore_success = test_backup.restore_backup(backup_path)
            backup_results["restore_tested"] = restore_success
            
            # Clean up test file
            if os.path.exists(test_db_path):
                os.remove(test_db_path)
            
            # Clean up old backups
            deleted_count = backup_manager.cleanup_old_backups(days_to_keep=1)
            backup_results["cleanup_performed"] = True
            backup_results["deleted_backups"] = deleted_count
            
        except Exception as e:
            logger.error(f"Backup/recovery test failed: {e}")
        
        return backup_results
    
    def run_comprehensive_optimization(self) -> Dict[str, Any]:
        """Run all database optimization tests."""
        logger.info("Starting comprehensive database optimization")
        
        # Run all optimization tests
        optimizations = {
            "connection_pooling": self.setup_connection_pooling(),
            "index_optimization": self.optimize_database_indexes(),
            "query_optimization": self.implement_query_optimization(),
            "database_monitoring": self.setup_database_monitoring(),
            "backup_recovery": self.configure_backup_and_recovery()
        }
        
        # Generate summary
        summary = self._generate_optimization_summary(optimizations)
        
        self.optimization_results["optimizations"] = optimizations
        self.optimization_results["summary"] = summary
        
        # Save results
        self._save_optimization_results()
        
        return self.optimization_results
    
    def _generate_optimization_summary(self, optimizations: Dict[str, Any]) -> Dict[str, Any]:
        """Generate summary of database optimization results."""
        summary = {
            "overall_performance_score": 0,
            "best_improvements": [],
            "recommendations": [],
            "total_optimizations": len(optimizations)
        }
        
        # Analyze connection pooling
        pooling = optimizations.get("connection_pooling", {})
        if pooling:
            best_pool = max(pooling.items(), 
                          key=lambda x: x[1].get("requests_per_second", 0))
            summary["best_improvements"].append(f"Connection pooling: {best_pool[0]} ({best_pool[1]['requests_per_second']:.1f} req/s)")
        
        # Analyze index optimization
        indexing = optimizations.get("index_optimization", {})
        total_improvement = 0
        index_count = 0
        for table_results in indexing.values():
            for query_improvement in table_results.get("query_improvements", {}).values():
                total_improvement += query_improvement.get("improvement_percent", 0)
                index_count += 1
        
        if index_count > 0:
            avg_improvement = total_improvement / index_count
            summary["best_improvements"].append(f"Index optimization: {avg_improvement:.1f}% average improvement")
        
        # Analyze query optimization
        query_opt = optimizations.get("query_optimization", {})
        total_query_improvement = 0
        query_count = 0
        for opt_result in query_opt.values():
            total_query_improvement += opt_result.get("improvement_percent", 0)
            query_count += 1
        
        if query_count > 0:
            avg_query_improvement = total_query_improvement / query_count
            summary["best_improvements"].append(f"Query optimization: {avg_query_improvement:.1f}% average improvement")
        
        # Analyze monitoring
        monitoring = optimizations.get("database_monitoring", {})
        if monitoring:
            test_results = monitoring.get("test_results", {})
            success_rate = test_results.get("success_rate", 0)
            if success_rate >= 95:
                summary["recommendations"].append("Database monitoring shows good performance")
            else:
                summary["recommendations"].append("Database monitoring indicates performance issues")
        
        # Calculate overall score
        score = 0
        if pooling:
            score += 20
        if indexing:
            score += 25
        if query_opt:
            score += 25
        if monitoring:
            score += 20
        if optimizations.get("backup_recovery", {}).get("backup_created"):
            score += 10
        
        summary["overall_performance_score"] = score
        
        return summary
    
    def _save_optimization_results(self):
        """Save optimization results to file."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"database_optimization_results_{timestamp}.json"
        
        with open(filename, 'w') as f:
            json.dump(self.optimization_results, f, indent=2, default=str)
        
        logger.info(f"Database optimization results saved to {filename}")


def main():
    """Main function to run database optimization."""
    print("üóÑÔ∏è Starting Database Optimization Analysis")
    print("=" * 50)
    
    # Initialize database optimizer
    optimizer = DatabaseOptimizer()
    
    try:
        # Run comprehensive optimization
        results = optimizer.run_comprehensive_optimization()
        
        # Print summary
        print("\nüìä Database Optimization Results Summary")
        print("=" * 50)
        
        summary = results["summary"]
        print(f"Overall Performance Score: {summary['overall_performance_score']}/100")
        print(f"Total Optimizations: {summary['total_optimizations']}")
        
        if summary["best_improvements"]:
            print("\nüèÜ Best Improvements:")
            for improvement in summary["best_improvements"]:
                print(f"  - {improvement}")
        
        if summary["recommendations"]:
            print("\nüí° Recommendations:")
            for recommendation in summary["recommendations"]:
                print(f"  - {recommendation}")
        
        print(f"\n‚úÖ Database optimization completed successfully!")
        print(f"üìÑ Detailed results saved to database_optimization_results_*.json")
        
    except Exception as e:
        logger.error(f"Database optimization failed: {e}")
        print(f"‚ùå Database optimization failed: {e}")


if __name__ == "__main__":
    main()
